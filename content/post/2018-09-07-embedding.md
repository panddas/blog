---
title: Word2vec、词向量、Embedding，这四个概念是什么
image: "img/portfolio/2018-9-7 20-49-7.jpg"
author: ''
date: '2018-09-07'
slug: embedding
categories:
  - code
tags:
  - keras
  - Tensorflow
  - rnn_lstm
  - python
  - r
---
>怎么是四个概念？明明只有三个词。因为Embedding是一语双关。同步学习tensorflow和keras，Embedding是遇到的第一个难点，各文献语焉不详，着实让我思考了几分钟。下面是我的收获...
<!--more-->

># 1. Word2vec

>用深度学习处理自然语言时(**就是用计算机处理中文文本时**)，tensorfow需要先使用Word2vec算法生成词向量(词向量的生成方法不止word2vec一种，但这比较常见)。那么词向量到底是什么呢？

># 2. 词向量

>我们还是用类比的方法来解释吧。计算机理解文字，其实跟人类差不多的。比如说人类如何理解“西瓜”，你知道它是绿色的，球形的，夏季才有的，切开里面是红色的，有黑色的籽，有充足的水份，水是甜的......也就是说，人类是通过一系列词的组合，来理解另一个词的。通过这一系列词的组合，人类可以将“西瓜”和“西红柿”区分开，就说明理解了“西瓜”。好，词向量我们已经理解了50%了。

>计算机当然不会直接处理文字，但是我们可以给字词编号，比如“西瓜”是007，绿色是028，球形是375，夏季是619(其它几个属性忽略了)，那么根据刚才的理解就有，007=028+375+619。当然了，这种写法不正规，写成向量的形式就是007=[028, 375, 619]。这就是词向量，等号左边是一个词，右边是一系列词组成的向量(精确点说就是三个自由度的词向量，也可以理解为有三个神经元)。计算机可以通过词向量来理解人类的语言，前提是人类发明了词向量。

>有许多东西不出现时，你根本不知道它有那么多妙用，这可能是一条真理。词向量就是最好的例子，它的用法超越了人类的经验。运用词向量，计算机可以通过四则运算进行推理。也就是说，可以将四则运算运用于文字，这个新鲜事物你尝试一下，解答下面的问题：

>**国王-男人+女人=？**

>这是一道简单的推理题，有没有发现，能力限制了你的想像力。计算机会告诉你：


>**<font color="ffffff">国王-男人+女人=皇后</font>**

>答案ctrl+a可见。通过这种方式，计算机对人类语言的理解程度可能会很可怕，毕竟它用电而你吃饭。

>当然，生成Embedding的运算过程是极其漫长的。等待过程中，你就会理解为什么有人愿意花5万大洋从Nvidia手里买特斯拉。

># 3. Embedding * 2

>回到主题，为什么说Embedding是一语双关呢？

>一方面，Embedding就是通过Word2vec等算法优化出来的词向量，所以如果你学习tensorflow做词向量时，记住你的最优化输出结果就是Embedding，后面可以接各种语义分析模型。

>另一方面，学习使用keras处理文本时会发现它创建了一个随机化的矩阵作为Embedding，就是这里容易把人搞糊涂。明明那tensorflow那里它是费时费力又费电好不容易优化出来的，为什么到keras这里变成随机生成了？而且最令人崩溃的是，使用随机的Embedding，学习精确度一样非常高，那当初为什么还要搞什么词向量，直接随机不就好了？

>是不是有种刚刚建立了某种信仰，分分种又被打脸的感觉？

>下面是理解的关键：记住，**你用keras时是有优化目标y的，其实keras在向目标y不断靠拢时，正是在不断优化Embedding矩阵。因此最终优化成功后，你的模型会包括这个Embedding矩阵在内。当然，它将不再是随机的，而是优化后的，此时的Embedding矩阵就相当于tensorflow的Embedding了，不再是一语双关。**

>好动脑筋的同学可能会问，能不能直接用tensorflow的Embedding代入keras，而不是随机生成？当然是可以的，不过keras还是会去优化这个代入的Embedding，毕竟生成这个Embedding的语料，并不是keras要处理的语料。

>
